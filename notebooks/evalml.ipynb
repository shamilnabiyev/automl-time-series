{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ff4983",
   "metadata": {},
   "source": [
    "# EvalML\n",
    "\n",
    "GitHub: https://github.com/alteryx/evalml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6401b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from evalml.automl import AutoMLSearch\n",
    "from evalml.problem_types.problem_types import ProblemTypes\n",
    "from evalml.pipelines import TimeSeriesBinaryClassificationPipeline\n",
    "from evalml.preprocessing import split_data\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c56e7e",
   "metadata": {},
   "source": [
    "## Generate a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07fd753f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 20), RangeIndex(start=0, stop=20, step=1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=10000)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "\n",
    "X.ww.init()\n",
    "\n",
    "X.shape, X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4fe8c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.298473</td>\n",
       "      <td>-1.218092</td>\n",
       "      <td>-0.477984</td>\n",
       "      <td>1.179264</td>\n",
       "      <td>-0.264375</td>\n",
       "      <td>-0.241809</td>\n",
       "      <td>-0.838345</td>\n",
       "      <td>-0.189705</td>\n",
       "      <td>1.509926</td>\n",
       "      <td>-0.572967</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.244651</td>\n",
       "      <td>1.525125</td>\n",
       "      <td>0.605920</td>\n",
       "      <td>0.695240</td>\n",
       "      <td>-0.590302</td>\n",
       "      <td>-0.665014</td>\n",
       "      <td>-0.008399</td>\n",
       "      <td>1.846055</td>\n",
       "      <td>0.349236</td>\n",
       "      <td>2010-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.940313</td>\n",
       "      <td>-0.483936</td>\n",
       "      <td>1.265297</td>\n",
       "      <td>-0.371966</td>\n",
       "      <td>-1.118780</td>\n",
       "      <td>0.706070</td>\n",
       "      <td>1.914420</td>\n",
       "      <td>-0.307438</td>\n",
       "      <td>-1.088141</td>\n",
       "      <td>-1.359594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171345</td>\n",
       "      <td>1.102213</td>\n",
       "      <td>0.058327</td>\n",
       "      <td>-0.052656</td>\n",
       "      <td>-1.849278</td>\n",
       "      <td>-1.071711</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.238361</td>\n",
       "      <td>0.963782</td>\n",
       "      <td>2010-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.311640</td>\n",
       "      <td>1.314532</td>\n",
       "      <td>-1.421946</td>\n",
       "      <td>0.686917</td>\n",
       "      <td>1.274491</td>\n",
       "      <td>-1.890640</td>\n",
       "      <td>0.956771</td>\n",
       "      <td>-1.654531</td>\n",
       "      <td>0.293637</td>\n",
       "      <td>-0.701893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226707</td>\n",
       "      <td>0.528948</td>\n",
       "      <td>1.852916</td>\n",
       "      <td>-1.768396</td>\n",
       "      <td>-0.143401</td>\n",
       "      <td>0.665558</td>\n",
       "      <td>-0.084680</td>\n",
       "      <td>0.383047</td>\n",
       "      <td>-0.474083</td>\n",
       "      <td>2010-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.378572</td>\n",
       "      <td>0.591799</td>\n",
       "      <td>1.340881</td>\n",
       "      <td>-0.228323</td>\n",
       "      <td>-0.688304</td>\n",
       "      <td>-1.277819</td>\n",
       "      <td>0.328915</td>\n",
       "      <td>-0.229157</td>\n",
       "      <td>1.662133</td>\n",
       "      <td>0.525428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.362785</td>\n",
       "      <td>0.295219</td>\n",
       "      <td>1.299650</td>\n",
       "      <td>3.123191</td>\n",
       "      <td>0.108567</td>\n",
       "      <td>-0.947054</td>\n",
       "      <td>0.222324</td>\n",
       "      <td>1.428925</td>\n",
       "      <td>-0.321512</td>\n",
       "      <td>2010-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.761131</td>\n",
       "      <td>0.844613</td>\n",
       "      <td>-1.865478</td>\n",
       "      <td>1.754670</td>\n",
       "      <td>0.217115</td>\n",
       "      <td>-1.315361</td>\n",
       "      <td>0.445070</td>\n",
       "      <td>-1.914010</td>\n",
       "      <td>0.933714</td>\n",
       "      <td>-1.104730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.898258</td>\n",
       "      <td>-0.309205</td>\n",
       "      <td>0.677011</td>\n",
       "      <td>-0.482433</td>\n",
       "      <td>0.538408</td>\n",
       "      <td>0.675033</td>\n",
       "      <td>0.304502</td>\n",
       "      <td>1.502091</td>\n",
       "      <td>-0.325404</td>\n",
       "      <td>2010-10-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.298473 -1.218092 -0.477984  1.179264 -0.264375 -0.241809 -0.838345   \n",
       "1 -0.940313 -0.483936  1.265297 -0.371966 -1.118780  0.706070  1.914420   \n",
       "2  0.311640  1.314532 -1.421946  0.686917  1.274491 -1.890640  0.956771   \n",
       "3 -0.378572  0.591799  1.340881 -0.228323 -0.688304 -1.277819  0.328915   \n",
       "4 -0.761131  0.844613 -1.865478  1.754670  0.217115 -1.315361  0.445070   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0 -0.189705  1.509926 -0.572967  ... -1.244651  1.525125  0.605920  0.695240   \n",
       "1 -0.307438 -1.088141 -1.359594  ...  0.171345  1.102213  0.058327 -0.052656   \n",
       "2 -1.654531  0.293637 -0.701893  ...  0.226707  0.528948  1.852916 -1.768396   \n",
       "3 -0.229157  1.662133  0.525428  ...  1.362785  0.295219  1.299650  3.123191   \n",
       "4 -1.914010  0.933714 -1.104730  ... -0.898258 -0.309205  0.677011 -0.482433   \n",
       "\n",
       "         15        16        17        18        19       date  \n",
       "0 -0.590302 -0.665014 -0.008399  1.846055  0.349236 2010-10-01  \n",
       "1 -1.849278 -1.071711  0.243545  0.238361  0.963782 2010-10-02  \n",
       "2 -0.143401  0.665558 -0.084680  0.383047 -0.474083 2010-10-03  \n",
       "3  0.108567 -0.947054  0.222324  1.428925 -0.321512 2010-10-04  \n",
       "4  0.538408  0.675033  0.304502  1.502091 -0.325404 2010-10-05  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['date'] = pd.date_range(\"2010-10-01\", periods=X.shape[0])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff0b362",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a086d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 21), (2000, 21), (8000,), (2000,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(X, y, problem_type=\"time series binary\", test_size=0.2)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f4b1b",
   "metadata": {},
   "source": [
    "## Experiment 1: Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22ec78c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/us3r/projects/automl-time-series/venv/lib/python3.8/site-packages/evalml/automl/automl_search.py:475: UserWarning:\n",
      "\n",
      "Time series support in evalml is still in beta, which means we are still actively building its core features. Please be mindful of that when running search().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "automl = AutoMLSearch(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    problem_type=\"time series binary\",\n",
    "    problem_configuration={\n",
    "        \"max_delay\": 5,\n",
    "        \"gap\": 0,\n",
    "        \"forecast_horizon\": 1,\n",
    "        \"time_index\": \"date\",\n",
    "        },\n",
    "    optimize_thresholds=True,\n",
    "    objective='f1',\n",
    "    verbose=True,\n",
    "    max_time=60*10, # time limit \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fb98723",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for F1. \n",
      "Greater score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Will stop searching for new pipelines after 600 seconds.\n",
      "\n",
      "Allowed model families: \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ec20a07332427a8ab415f5c4f07908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Best Score',\n",
       "              'type'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline Pipeline: Time Series Baseline Binary Pipeline\n",
      "Time Series Baseline Binary Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.493\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Logistic Regression Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.665\n",
      "Random Forest Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.550\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 2 *\n",
      "*****************************\n",
      "\n",
      "Logistic Regression Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.665\n",
      "Random Forest Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.550\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 3 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.659\n",
      "LightGBM Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.642\n",
      "Extra Trees Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.668\n",
      "Elastic Net Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.665\n",
      "CatBoost Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.668\n",
      "XGBoost Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.657\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 4 *\n",
      "*****************************\n",
      "\n",
      "Extra Trees Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.668\n",
      "CatBoost Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.612\n",
      "Logistic Regression Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.665\n",
      "Extra Trees Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.668\n",
      "CatBoost Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.592\n",
      "Logistic Regression Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.662\n",
      "Extra Trees Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.668\n",
      "CatBoost Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.587\n",
      "Logistic Regression Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean F1: 0.656\n",
      "\n",
      "Search finished after 10:22            \n",
      "Best pipeline: Extra Trees Classifier w/ Label Encoder + Imputer + Time Series Featurizer + DateTime Featurizer + Drop NaN Rows Transformer\n",
      "Best pipeline F1: 0.667686\n",
      "\n",
      "CPU times: user 27min 12s, sys: 1min 56s, total: 29min 9s\n",
      "Wall time: 10min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "automl.search()\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6c9f8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1004\n",
      "           1       0.50      1.00      0.66       996\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.25      0.50      0.33      2000\n",
      "weighted avg       0.25      0.50      0.33      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/us3r/projects/automl-time-series/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/home/us3r/projects/automl-time-series/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/home/us3r/projects/automl-time-series/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = automl.best_pipeline.predict(X=X_test, objective='f1', X_train=X_train, y_train=y_train) * 1\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406849af",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.best_pipeline.score(X=X_test, y=y_test, objectives=['f1'], X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.objectives import get_all_objective_names\n",
    "print(get_all_objective_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f1291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.describe_pipeline(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95762a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = automl.best_pipeline\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline_score = pl.score(X_test, y_test, ['f1'], X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083357d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee8b28",
   "metadata": {},
   "source": [
    "## Experiment 2: Human Activity Recognition - HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source: https://www.kaggle.com/datasets/uciml/human-activity-recognition-with-smartphones\n",
    "train = pd.read_csv(\"../data/human-activity-recognition/train.csv\")\n",
    "\n",
    "# Convert labels to binary\n",
    "train['Activity'] = pd.DataFrame(np.where(train['Activity']=='WALKING_DOWNSTAIRS', 1, 0))\n",
    "\n",
    "# Create a new date column\n",
    "train['date'] = pd.date_range(start='1/1/2022', periods=len(train), freq='S')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e19cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max value will be incremented by 1 second and used in test dataset (see below)\n",
    "train['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Activity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[train.columns.difference(['subject', 'Activity'])]\n",
    "X.ww.init()\n",
    "\n",
    "X_train = X[0:800]\n",
    "X_val = X[800:200]\n",
    "\n",
    "y_train = train['Activity'][0:800]\n",
    "y_val = train['Activity'][800:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6846cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train)\n",
    "plt.title('Histogram of activites')\n",
    "plt.xticks(rotation = 90) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.demos import load_weather\n",
    "from evalml.automl import AutoMLSearch\n",
    "from evalml.utils.gen_utils import validate_holdout_datasets\n",
    "from evalml.problem_types.problem_types import ProblemTypes\n",
    "import woodwork as ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a69228",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_config = {'gap': 0, \n",
    "                  'max_delay': 10, \n",
    "                  'forecast_horizon': 1, \n",
    "                  'time_index': 'date'\n",
    "                 }\n",
    "\n",
    "# model_families=['xgboost', 'random_forest', 'linear_model', 'extra_trees','decision_tree']\n",
    "# model_families=[ModelFamily.XGBOOST, ModelFamily.LIGHTGBM, ModelFamily.CATBOOST]\n",
    "model_families=[ModelFamily.XGBOOST]\n",
    "\n",
    "automl = AutoMLSearch(X_train, \n",
    "                      y_train, \n",
    "                      problem_type=ProblemTypes.TIME_SERIES_BINARY,\n",
    "                      max_batches=1,\n",
    "                      problem_configuration=problem_config,\n",
    "                      max_time=60*10, # limit the pipeline search duration\n",
    "                      allowed_model_families=model_families,\n",
    "                      objective='f1',\n",
    "                      sampler_method=None, \n",
    "                      verbose=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be83872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "automl.search()\n",
    "\n",
    "print('') # Started at 13:23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fdce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/human-activity-recognition/test.csv\")\n",
    "test['Activity'] = pd.DataFrame(np.where(test['Activity']=='WALKING_DOWNSTAIRS', 1, 0))\n",
    "test['date'] = pd.date_range(start='1/1/2022 02:02:32', periods=len(test), freq='S')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 562 features. Ignore the 'subject' column\n",
    "X_test = test[test.columns.difference(['subject', 'Activity'])]\n",
    "# X_test['date'] = pd.date_range(start='1/1/2022 02:02:32', periods=len(X_test), freq='S')\n",
    "# Select the 'activity' column as an outcome\n",
    "y_test = test['Activity'] #.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = automl.best_pipeline\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline_score = pl.score(X_test, y_test, ['f1'], X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183e1b7",
   "metadata": {},
   "source": [
    "TODO: try to debug the code and find out the reason of the following exception:\n",
    "\n",
    "\n",
    "```\n",
    "PipelineScoreError: F1 encountered AttributeError with message ('NoneType' object has no attribute 'iloc'):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858263b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc_pipeline = TimeSeriesBinaryClassificationPipeline(\n",
    "    # component_graph=[], \n",
    "    component_graph = ['Logistic Regression Classifier'],\n",
    "    parameters={\"pipeline\": {\"gap\": 0, \"max_delay\": 1, \"forecast_horizon\": 1, \"time_index\": \"date\"}}, \n",
    "    custom_name=None, \n",
    "    random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7df81",
   "metadata": {},
   "source": [
    "**ValueError**: no such file ../automl-time-series/venv/lib/python3.8/site-packages/prophet/stan_model/prophet_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "races =   [\"asian\",\"black\",\"hispanic\",\"other\",\"white\"]\n",
    "\n",
    "# Generate random data\n",
    "voter_race = np.random.choice(a= races,\n",
    "                              p = [0.05, 0.15 ,0.25, 0.05, 0.5],\n",
    "                              size=1000)\n",
    "\n",
    "voter_age = stats.poisson.rvs(loc=18,\n",
    "                              mu=30,\n",
    "                              size=1000)\n",
    "\n",
    "# Group age data by race\n",
    "voter_frame = pd.DataFrame({\"race\":voter_race,\"age\":voter_age})\n",
    "groups = voter_frame.groupby(\"race\").groups\n",
    "\n",
    "# Etract individual groups\n",
    "asian = voter_age[groups[\"asian\"]]\n",
    "black = voter_age[groups[\"black\"]]\n",
    "hispanic = voter_age[groups[\"hispanic\"]]\n",
    "other = voter_age[groups[\"other\"]]\n",
    "white = voter_age[groups[\"white\"]]\n",
    "\n",
    "# Perform the ANOVA\n",
    "stats.f_oneway(asian, black, hispanic, other, white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec651d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(asian),\\\n",
    "np.mean(black),\\\n",
    "np.mean(hispanic),\\\n",
    "np.mean(other),\\\n",
    "np.mean(white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "# Generate random data\n",
    "voter_race = np.random.choice(a= races,\n",
    "                              p = [0.05, 0.15 ,0.25, 0.05, 0.5],\n",
    "                              size=1000)\n",
    "\n",
    "# Use a different distribution for white ages\n",
    "white_ages = stats.poisson.rvs(loc=18, \n",
    "                              mu=32,\n",
    "                              size=1000)\n",
    "\n",
    "voter_age = stats.poisson.rvs(loc=18,\n",
    "                              mu=30,\n",
    "                              size=1000)\n",
    "\n",
    "voter_age = np.where(voter_race==\"white\", white_ages, voter_age)\n",
    "\n",
    "# Group age data by race\n",
    "voter_frame = pd.DataFrame({\"race\":voter_race,\"age\":voter_age})\n",
    "groups = voter_frame.groupby(\"race\").groups   \n",
    "\n",
    "# Extract individual groups\n",
    "asian = voter_age[groups[\"asian\"]]\n",
    "black = voter_age[groups[\"black\"]]\n",
    "hispanic = voter_age[groups[\"hispanic\"]]\n",
    "other = voter_age[groups[\"other\"]]\n",
    "white = voter_age[groups[\"white\"]]\n",
    "\n",
    "# Perform the ANOVA\n",
    "stats.f_oneway(asian, black, hispanic, other, white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(asian),\\\n",
    "np.mean(black),\\\n",
    "np.mean(hispanic),\\\n",
    "np.mean(other),\\\n",
    "np.mean(white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0741b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_ages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
