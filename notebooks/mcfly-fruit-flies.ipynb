{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c27ad72",
   "metadata": {},
   "source": [
    "# Mcfly using tensorflow.data.Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf6798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from sktime.datatypes import convert_to\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.datasets import load_from_arff_to_dataframe\n",
    "from mcfly.modelgen import generate_models\n",
    "from mcfly.find_architecture import train_models_on_samples\n",
    "from mcfly.find_architecture import find_best_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e2f4f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f56c7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0       0.000244\n",
       "1       0.001831\n",
       "2      -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0      -0.000244\n",
       "1       0.000275\n",
       "2      -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0       0.002350\n",
       "1      -0.002441\n",
       "2      -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0       0.001465\n",
       "1       0.001221\n",
       "2      -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0       0.000397\n",
       "1      -0.000061\n",
       "2      -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               dim_0\n",
       "0  0       0.000244\n",
       "1       0.001831\n",
       "2      -0.00...\n",
       "1  0      -0.000244\n",
       "1       0.000275\n",
       "2      -0.00...\n",
       "2  0       0.002350\n",
       "1      -0.002441\n",
       "2      -0.00...\n",
       "3  0       0.001465\n",
       "1       0.001221\n",
       "2      -0.00...\n",
       "4  0       0.000397\n",
       "1      -0.000061\n",
       "2      -0.00..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"C:/Users/NABS/Downloads/FruitFlies/\"\n",
    "X, y = load_from_arff_to_dataframe(\n",
    "    os.path.join(DATA_PATH, \"FruitFlies_TRAIN.arff\")\n",
    ")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6882eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['melanogaster', 'suzukii', 'zaprionus'], dtype='<U12')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f80bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_np = OneHotEncoder().fit_transform(y.reshape(-1, 1)).toarray()\n",
    "y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe3d5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17259, 1, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np = convert_to(X, to_type=\"numpy3D\")\n",
    "X_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2663e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17259, 5000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np = np.swapaxes(X_np, 1, 2)\n",
    "X_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c3171b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.38 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for i in range(X_np.shape[0]):\n",
    "    with open(os.path.join(DATA_PATH, \"instances\", f\"instance_{i}_label_{y[i]}_.npy\"), 'wb') as f:\n",
    "        np.save(f, X_np[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01698932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np[0, np.newaxis, :, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8aad4",
   "metadata": {},
   "source": [
    "## tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b7ddd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zaprionus', 'zaprionus', 'zaprionus', 'zaprionus', 'suzukii']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"C:/Users/NABS/Downloads/FruitFlies/\"\n",
    "\n",
    "file_names = os.listdir(os.path.join(DATA_PATH, \"instances\"))\n",
    "random.shuffle(file_names)\n",
    "\n",
    "labels = map(lambda x: x.split(\"_\")[3], file_names)\n",
    "labels = list(labels)\n",
    "\n",
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "308d1240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['instance_9530_label_zaprionus_.npy', 'instance_12530_label_zaprionus_.npy'],\n",
       " ['zaprionus', 'zaprionus'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names[0:2], labels[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b73016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, train_label, val_label = train_test_split(\n",
    "    file_names, \n",
    "    labels, \n",
    "    train_size=0.80, \n",
    "    random_state=42, \n",
    "    stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2acc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_set = set(labels)\n",
    "labels_int = list(range(len(labels_set)))\n",
    "labels_dict = dict(zip(labels_set, labels_int))\n",
    "NUM_CLASSES = len(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9df0e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_generator(data_set):\n",
    "    def get_int_label(file_name):\n",
    "        string_label = file_name.split(\"_\")[3]\n",
    "        return labels_dict[string_label]\n",
    "    \n",
    "    def get_x_y(file_name):\n",
    "        x = np.load(os.path.join(DATA_PATH, \"instances\", file_name))\n",
    "        y = get_int_label(file_name)\n",
    "        y = tf.constant(y)\n",
    "        y = tf.one_hot(y, NUM_CLASSES)\n",
    "        return x, y\n",
    "    \n",
    "    def func(i):\n",
    "        idx = i.numpy() # Decoding from the EagerTensor object\n",
    "        x, y = get_x_y(data_set[idx])\n",
    "        return x, y\n",
    "\n",
    "    z = list(range(len(data_set)))\n",
    "    dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint16)\n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=len(z), seed=0,  \n",
    "                              reshuffle_each_iteration=True)\n",
    "    dataset = dataset.map(lambda i: tf.py_function(func=func, \n",
    "                                                   inp=[i], \n",
    "                                                   Tout=[tf.float32,\n",
    "                                                         tf.float32]\n",
    "                                                   ), \n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(8)#.map(_fixup_shape)\n",
    "    # dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_generator = get_dataset_generator(train_set)\n",
    "val_generator = get_dataset_generator(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88de59fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5000, 1) tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x_tmp, y_tmp in val_generator.take(1):\n",
    "    print(x_tmp.shape, y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3857067",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_params, best_model_type, knn_acc = \\\n",
    "            find_best_architecture(\n",
    "                X_train=train_generator,\n",
    "                y_train=None,\n",
    "                X_val=val_generator,\n",
    "                y_val=None\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a50212",
   "metadata": {},
   "source": [
    "## Using in-memory dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3dae35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13807, 5000, 1), (3452, 5000, 1), (13807, 3), (3452, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_np, \n",
    "    y_np, \n",
    "    train_size=0.80, \n",
    "    random_state=42, \n",
    "    stratify=y_np\n",
    ")\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d308e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_params, best_model_type, knn_acc = \\\n",
    "            find_best_architecture(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_val=X_val,\n",
    "                y_val=y_val\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e035f7",
   "metadata": {},
   "source": [
    "ResourceExhaustedError: OOM when allocating tensor with shape[405000,1704] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b138a",
   "metadata": {},
   "source": [
    "## tensorflow Dataset from Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ab65917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13807, 5000, 1), (3452, 5000, 1), (13807, 3), (3452, 3))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_np, \n",
    "    y_np, \n",
    "    train_size=0.80, \n",
    "    random_state=42, \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c977b0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13807, 5000, 1), (3452, 5000, 1), (13807, 3), (3452, 3))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f68013ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_params, best_model_type, knn_acc = \\\n",
    "            find_best_architecture(\n",
    "                X_train=train_dataset,\n",
    "                y_train=None,\n",
    "                X_val=val_dataset,\n",
    "                y_val=None\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c25c70",
   "metadata": {},
   "source": [
    "AttributeError: 'TensorSliceDataset' object has no attribute 'shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19042f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1) tf.Tensor([0. 1. 0.], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for x0, y0 in train_dataset.take(1): \n",
    "    print(x0.shape, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c485065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23003"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = generate_models(x_shape=(13807, 5000, 1), \n",
    "                         number_of_classes=3,\n",
    "                         number_of_models=2,\n",
    "                         metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a3e733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models will be trained on subset of the data (subset size: 100).\n",
      "Training model 0 DeepConvLSTM\n",
      "Epoch 1/5\n",
      "   6/1726 [..............................] - ETA: 11:51 - loss: 2.1406 - accuracy: 0.3750"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nFailed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 23, 80, 1, 5000, 8, 80] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall_1]] [Op:__inference_train_function_83995]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m histories, val_accuracies, val_losses \u001b[38;5;241m=\u001b[39m\\\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_models_on_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# batch_size=5,\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m#nr_epochs=20,\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m#subset_size=1000,\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m#outputfile=outputfile,\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitProjects\\automl-time-series\\venv\\lib\\site-packages\\mcfly\\find_architecture.py:174\u001b[0m, in \u001b[0;36mtrain_models_on_samples\u001b[1;34m(X_train, y_train, X_val, y_val, models, nr_epochs, subset_size, verbose, outputfile, model_path, early_stopping_patience, batch_size, metric, class_weight)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 174\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnr_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# see comment on subsize_set\u001b[39;49;00m\n\u001b[0;32m    177\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m histories\u001b[38;5;241m.\u001b[39mappend(history)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric_name \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmetrics_names:\n",
      "File \u001b[1;32m~\\Documents\\GitProjects\\automl-time-series\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Documents\\GitProjects\\automl-time-series\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInternalError\u001b[0m: Graph execution error:\n\nFailed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 23, 80, 1, 5000, 8, 80] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall_1]] [Op:__inference_train_function_83995]"
     ]
    }
   ],
   "source": [
    "histories, val_accuracies, val_losses =\\\n",
    "    train_models_on_samples(train_dataset.batch(8),\n",
    "                            None,\n",
    "                            val_dataset.batch(8),\n",
    "                            None,\n",
    "                            models,\n",
    "                            # batch_size=5,\n",
    "                            #nr_epochs=20,\n",
    "                            #subset_size=1000,\n",
    "                            verbose=True,\n",
    "                            #outputfile=outputfile,\n",
    "                            metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778e498",
   "metadata": {},
   "source": [
    "## Keras data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddada66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suzukii': 0, 'melanogaster': 1, 'zaprionus': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7a3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = {}\n",
    "for file_name in train_set:\n",
    "    lab = file_name.split(\"_\")[3]\n",
    "    lab = labels_dict[lab]\n",
    "    new_labels[file_name] = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "05e3429c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1, 5000, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((8, *(1, 5000), 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b912380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=5000, n_channels=1,\n",
    "                 n_classes=3, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            DATA_PATH = \"C:/Users/NABS/Downloads/FruitFlies/\"\n",
    "            X[i, ] = np.load(os.path.join(DATA_PATH, \"instances\", ID))\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, tf.keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e24ff77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtmp = np.empty((8, 5000, 1))\n",
    "Xtmp[0, ] = np.load(os.path.join(DATA_PATH, \"instances\", 'instance_13556_label_zaprionus_.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "59c72542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5000, 1)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "768f4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': 5000,\n",
    "          'batch_size': 3,\n",
    "          'n_classes': 3,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(train_set, new_labels, **params)\n",
    "validation_generator = DataGenerator(val_set, new_labels, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcde1f8",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f644a846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NABS\\Documents\\GitProjects\\automl-time-series\\venv\\lib\\site-packages\\mcfly\\modelgen.py:86: UserWarning: Specified number_of_models is smaller than the given number of model types.\n",
      "  warnings.warn(\"Specified number_of_models is smaller than the given number of model types.\")\n",
      "C:\\Users\\NABS\\Documents\\GitProjects\\automl-time-series\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "models = generate_models(x_shape=(None, 5000, 1), \n",
    "                         number_of_classes=3,\n",
    "                         number_of_models=2,\n",
    "                         metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f228f018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<keras.engine.functional.Functional at 0x13db763f310>,\n",
       "  {'learning_rate': 0.006191830789697886,\n",
       "   'regularization_rate': 0.02271788149250915,\n",
       "   'network_depth': 4,\n",
       "   'filters_number': 63,\n",
       "   'max_kernel_size': 90},\n",
       "  'InceptionTime'),\n",
       " (<keras.engine.functional.Functional at 0x13dbf051b50>,\n",
       "  {'learning_rate': 0.006649371027656127,\n",
       "   'regularization_rate': 0.00016364120810765338,\n",
       "   'network_depth': 5,\n",
       "   'min_filters_number': 92,\n",
       "   'max_kernel_size': 28},\n",
       "  'ResNet')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca1de07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m histories, val_accuracies, val_losses \u001b[38;5;241m=\u001b[39m\\\n\u001b[1;32m----> 2\u001b[0m     train_models_on_samples(\u001b[43mtraining_generator\u001b[49m,\n\u001b[0;32m      3\u001b[0m                             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m                             validation_generator,\n\u001b[0;32m      5\u001b[0m                             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m                             models,\n\u001b[0;32m      7\u001b[0m                             \u001b[38;5;66;03m# batch_size=5,\u001b[39;00m\n\u001b[0;32m      8\u001b[0m                             \u001b[38;5;66;03m#nr_epochs=20,\u001b[39;00m\n\u001b[0;32m      9\u001b[0m                             \u001b[38;5;66;03m#subset_size=1000,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m                             \u001b[38;5;66;03m#outputfile=outputfile,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m                             metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_generator' is not defined"
     ]
    }
   ],
   "source": [
    "histories, val_accuracies, val_losses =\\\n",
    "    train_models_on_samples(training_generator,\n",
    "                            None,\n",
    "                            validation_generator,\n",
    "                            None,\n",
    "                            models,\n",
    "                            # batch_size=5,\n",
    "                            #nr_epochs=20,\n",
    "                            #subset_size=1000,\n",
    "                            verbose=True,\n",
    "                            #outputfile=outputfile,\n",
    "                            metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_params, best_model_type, knn_acc = \\\n",
    "            find_best_architecture(\n",
    "                X_train=training_generator,\n",
    "                y_train=None,\n",
    "                X_val=validation_generator,\n",
    "                y_val=None, \n",
    "                number_of_models=5,\n",
    "                nr_epochs=20\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1535192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
